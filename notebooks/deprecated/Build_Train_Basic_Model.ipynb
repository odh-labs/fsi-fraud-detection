{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JupyterHub Notebook\n",
    "\n",
    "### This notebook server is hosted on the OpenShift platform which provides a separate server for individual user. The platform take care about the provisioning of the server and allocating related to storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Minio in /opt/app-root/lib/python3.6/site-packages (5.0.10)\n",
      "Requirement already satisfied: certifi in /opt/app-root/lib/python3.6/site-packages (from Minio) (2019.6.16)\n",
      "Requirement already satisfied: pytz in /opt/app-root/lib/python3.6/site-packages (from Minio) (2019.2)\n",
      "Requirement already satisfied: urllib3 in /opt/app-root/lib/python3.6/site-packages (from Minio) (1.25.3)\n",
      "Requirement already satisfied: python-dateutil in /opt/app-root/lib/python3.6/site-packages (from Minio) (2.8.0)\n",
      "Requirement already satisfied: configparser in /opt/app-root/lib/python3.6/site-packages (from Minio) (3.7.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/app-root/lib/python3.6/site-packages (from python-dateutil->Minio) (1.12.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1 is available.\n",
      "You should consider upgrading via the '/opt/app-root/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install Minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minio import Minio\n",
    "from minio.error import ResponseError\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow import feature_column\n",
    "from tensorboard.plugins.hparams import api as hp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_s3_server():\n",
    "    minioClient = Minio('minio-ml-workshop:9000',\n",
    "                    access_key='minio',\n",
    "                    secret_key='minio123',\n",
    "                    secure=False)\n",
    "\n",
    "    return minioClient\n",
    "\n",
    "def upload_learning_stats_to_s3(folder_name):\n",
    "    minioClient = get_s3_server()\n",
    "\n",
    "    files = []\n",
    "    for r, d, f in os.walk(folder_name):\n",
    "        for file in f:\n",
    "            files.append(os.path.join(r, file))\n",
    "\n",
    "    for f in files:\n",
    "        print(f)\n",
    "        minioClient.fput_object(bucket_name='model-stats', object_name=\"tensordata/\"  + f , file_path='./' + f)\n",
    "\n",
    "\n",
    "#%%\n",
    "\n",
    "def download_all_files(bucket_name):\n",
    "    minioClient = get_s3_server()\n",
    "    objects = minioClient.list_objects_v2(bucket_name=bucket_name,\n",
    "                                          recursive=True)\n",
    "    for obj in objects:\n",
    "        # print(obj.bucket_name, obj.object_name.encode('utf-8'), obj.last_modified,\n",
    "        #       obj.etag, obj.size, obj.content_type)\n",
    "        try:\n",
    "            minioClient.fget_object(obj.bucket_name, obj.object_name,\n",
    "                                          '/tmp/' + os.path.basename(obj.object_name))\n",
    "            # print(minioClient.fget_object(obj.bucket_name, obj.object_name,\n",
    "            #                               '/tmp/' + os.path.basename(obj.object_name)))\n",
    "        except ResponseError as err:\n",
    "            print(err)\n",
    "\n",
    "\n",
    "#%%\n",
    "\n",
    "def get_run_logdir(root_logdir):\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "\n",
    "def deploy_model(model_name):\n",
    "    minioClient = get_s3_server()\n",
    "\n",
    "    files = []\n",
    "    folder_name = model_name\n",
    "    for r, d, f in os.walk(folder_name):\n",
    "        for file in f:\n",
    "                files.append(os.path.join(r, file))\n",
    "\n",
    "    for f in files:\n",
    "        print(f)\n",
    "        minioClient.fput_object(bucket_name='models', object_name=f , file_path='./' + f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch Data from S3 bucket hosted on the OpenShift Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4-tf\n",
      "2.1.0\n",
      "Data Fetched\n"
     ]
    }
   ],
   "source": [
    "root_logdir = \".\"\n",
    "\n",
    "# https://www.tensorflow.org/tensorboard/hyperparameter_tuning_with_hparams\n",
    "\n",
    "\n",
    "\n",
    "print(tf.keras.__version__)\n",
    "print(tf.__version__)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "csv_columns = [\"row_number\", \"Time\", \"V1\", \"V2\", \"V3\", \"V4\", \"V5\", \"V6\", \"V7\", \"V8\", \"V9\", \"V10\", \"V11\", \"V12\", \"V13\", \"V14\", \"V15\",\n",
    "               \"V16\", \"V17\",  \"Amount\", \"Class\", \"V18\", \"V19\", \"V20\", \"V21\", \"V22\", \"V23\", \"V24\", \"V25\", \"V26\", \"V27\", \"V28\" ]\n",
    "\n",
    "download_all_files('data')\n",
    "\n",
    "source = '/tmp'\n",
    "for file in os.listdir(source):\n",
    "    if 'part-0011' in file:\n",
    "        old_file = os.path.join(\"/tmp\", file)\n",
    "        new_file = os.path.join(\"/tmp\", \"test-\" + file)\n",
    "        os.rename(old_file, new_file)\n",
    "        continue\n",
    "    if 'part-0012' in file:\n",
    "        old_file = os.path.join(\"/tmp\", file)\n",
    "        new_file = os.path.join(\"/tmp\", \"validate-\" + file)\n",
    "        os.rename(old_file, new_file)\n",
    "        continue\n",
    "\n",
    "\n",
    "#%%\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "csv_files = \"/tmp/*.csv\"\n",
    "dataset = tf.data.experimental.make_csv_dataset(csv_files, column_names=csv_columns, batch_size=1024, header=False,\n",
    "                                                label_name=\"Class\",\n",
    "                                                select_columns=[\"V1\", \"V2\", \"V3\", \"V4\", \"V5\", \"V6\", \"V7\", \"V8\", \"V9\",\n",
    "                                                                \"V10\", \"V11\", \"V12\", \"V13\", \"V14\", \"V15\", \"V16\", \"V17\",\n",
    "                                                                \"V18\", \"V19\", \"V20\", \"V21\", \"V22\", \"V23\", \"V24\", \"V25\",\n",
    "                                                                \"V26\", \"V27\", \"V28\", \"Amount\", \"Class\"])\n",
    "test_dataset = tf.data.experimental.make_csv_dataset(\"/tmp/test-*.csv\", batch_size=1024, column_names=csv_columns, header=False,\n",
    "                                                label_name=\"Class\",\n",
    "                                                select_columns=[\"V1\", \"V2\", \"V3\", \"V4\", \"V5\", \"V6\", \"V7\", \"V8\", \"V9\",\n",
    "                                                                \"V10\", \"V11\", \"V12\", \"V13\", \"V14\", \"V15\", \"V16\", \"V17\",\n",
    "                                                                \"V18\", \"V19\", \"V20\", \"V21\", \"V22\", \"V23\", \"V24\", \"V25\",\n",
    "                                                                \"V26\", \"V27\", \"V28\", \"Amount\", \"Class\"])\n",
    "\n",
    "validate_dataset = tf.data.experimental.make_csv_dataset(\"/tmp/validate-*.csv\", batch_size=1024, column_names=csv_columns, header=False,\n",
    "                                                label_name=\"Class\",\n",
    "                                                select_columns=[\"V1\", \"V2\", \"V3\", \"V4\", \"V5\", \"V6\", \"V7\", \"V8\", \"V9\",\n",
    "                                                                \"V10\", \"V11\", \"V12\", \"V13\", \"V14\", \"V15\", \"V16\", \"V17\",\n",
    "                                                                \"V18\", \"V19\", \"V20\", \"V21\", \"V22\", \"V23\", \"V24\", \"V25\",\n",
    "                                                                \"V26\", \"V27\", \"V28\", \"Amount\", \"Class\"])\n",
    "\n",
    "feature_columns = []\n",
    "for fc in [\"V1\", \"V2\", \"V3\", \"V4\", \"V5\", \"V6\", \"V7\", \"V8\", \"V9\",\n",
    "           \"V10\", \"V11\", \"V12\", \"V13\", \"V14\", \"V15\", \"V16\", \"V17\",\n",
    "           \"V18\", \"V19\", \"V20\", \"V21\", \"V22\", \"V23\", \"V24\", \"V25\",\n",
    "           \"V26\", \"V27\", \"V28\", \"Amount\"]:\n",
    "    feature_columns.append(feature_column.numeric_column(fc, shape=()))\n",
    "\n",
    "\n",
    "print(\"Data Fetched\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(keras.layers.DenseFeatures(feature_columns))\n",
    "model.add(keras.layers.Dense(50,   activation=\"tanh\"))\n",
    "keras.layers.Dropout(rate=0.2),\n",
    "model.add(keras.layers.Dense(50, activation=\"tanh\"))\n",
    "keras.layers.Dropout(rate=0.2),\n",
    "model.add(keras.layers.Dense(20, activation=\"tanh\"))\n",
    "keras.layers.Dropout(rate=0.2),\n",
    "# 1 becuase the output is fraud or not\n",
    "model.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train your model\n",
    "### The CPU and GPU and Machine is alloated on demand by the OpenShift Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 273.4375 steps, validate for 3.90625 steps\n",
      "274/273 [==============================] - 16s 58ms/step - loss: 0.0059 - accuracy: 0.9954 - val_loss: 2.2944e-04 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = 280000 / 1024\n",
    "validation_steps = 4000/1024\n",
    "\n",
    "# tensor board\n",
    "run_logdir = get_run_logdir(root_logdir)\n",
    "\n",
    "# tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(log_dir=run_logdir,\n",
    "                                             histogram_freq=2, write_graph=True, write_images=True,\n",
    "                                             update_freq='epoch', profile_batch=2, embeddings_freq=0,\n",
    "                                             embeddings_metadata=None)\n",
    "\n",
    "history = model.fit(dataset, validation_data=validate_dataset, validation_steps=validation_steps,   \n",
    "                    steps_per_epoch=steps_per_epoch, epochs=1, callbacks=[tensorboard_cb])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate your model and measure accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/3 [==============================] - 1s 138ms/step - loss: 0.0015 - accuracy: 0.9988\n",
      "test loss, test acc: [0.0014899923466145993, 0.9987793]\n"
     ]
    }
   ],
   "source": [
    "evaluation_result = model.evaluate(test_dataset, steps= validation_steps)\n",
    "print('test loss, test acc:', evaluation_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload Model to Visualise its internals. The visualisation server is hosted on OpenShift Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_2020_05_01-04_59_51/validation/events.out.tfevents.1588309208.jupyterhub-nb-fmasood.38.77777.v2\n",
      "run_2020_05_01-04_59_51/train/events.out.tfevents.1588309192.jupyterhub-nb-fmasood.38.74350.v2\n",
      "run_2020_05_01-04_59_51/train/events.out.tfevents.1588309193.jupyterhub-nb-fmasood.profile-empty\n",
      "run_2020_05_01-04_59_51/train/plugins/profile/2020-05-01_04-59-53/local.trace\n"
     ]
    }
   ],
   "source": [
    "upload_learning_stats_to_s3(run_logdir.replace(\"./\", \"\", 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload the model to be used as an API. The API Server is hosted by OpenShift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/app-root/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: ./test_model/1/assets\n",
      "test_model/1/saved_model.pb\n",
      "test_model/1/variables/variables.index\n",
      "test_model/1/variables/variables.data-00000-of-00001\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(model, \"./test_model/1\", signatures=None)\n",
    "deploy_model(\"test_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
