{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JupyterHub Notebook\n",
    "\n",
    "### This notebook server is hosted on the OpenShift platform which provides a separate server for individual user. The platform take care about the provisioning of the server and allocating related to storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from tensorboard.plugins.hparams import api as hp_api\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from minio import Minio\n",
    "from minio.error import ResponseError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_s3_server():\n",
    "    minioClient = Minio('minio-ml-workshop:9000',\n",
    "                    access_key='minio',\n",
    "                    secret_key='minio123',\n",
    "                    secure=False)\n",
    "\n",
    "    return minioClient\n",
    "\n",
    "def download_all_files(bucket_name):\n",
    "    minioClient = get_s3_server()\n",
    "    objects = minioClient.list_objects_v2(bucket_name=bucket_name,\n",
    "                                          recursive=True)\n",
    "    for obj in objects:\n",
    "        print(obj.bucket_name, obj.object_name.encode('utf-8'), obj.last_modified,\n",
    "              obj.etag, obj.size, obj.content_type)\n",
    "        try:\n",
    "            print(minioClient.fget_object(obj.bucket_name, obj.object_name,\n",
    "                                          '/tmp/' + os.path.basename(obj.object_name)))\n",
    "        except ResponseError as err:\n",
    "            print(err)\n",
    "\n",
    "#%%\n",
    "\n",
    "def load_card_data(file_path):\n",
    "    csv_path = os.path.join(file_path, \"creditcard.csv\")\n",
    "    # return pd.read_csv(csv_path, header=None)\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "#%%\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "def upload_learning_stats_to_s3(folder_name):\n",
    "    minioClient = get_s3_server()\n",
    "\n",
    "    files = []\n",
    "    for r, d, f in os.walk(folder_name):\n",
    "        for file in f:\n",
    "            files.append(os.path.join(r, file))\n",
    "\n",
    "    for f in files:\n",
    "        print(f)\n",
    "        minioClient.fput_object(bucket_name='model-stats', object_name=\"tensordata/\"  + f , file_path='./' + f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch Data from S3 Bucket - Hosted on OpenShift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rawdata b'creditcard.csv' 2020-04-30 03:48:55.835000+00:00 e90efcb83d69faf99fcab8b0255024de 150828752 None\n",
      "<Object: bucket_name: rawdata object_name: b'creditcard.csv' last_modified: time.struct_time(tm_year=2020, tm_mon=4, tm_mday=30, tm_hour=3, tm_min=48, tm_sec=55, tm_wday=3, tm_yday=121, tm_isdst=0) etag: e90efcb83d69faf99fcab8b0255024de size: 150828752 content_type: text/csv, is_dir: False, metadata: {'Content-Type': 'text/csv'}>\n"
     ]
    }
   ],
   "source": [
    "download_all_files('rawdata')\n",
    "file_path = \"/tmp\"\n",
    "full_card_data = load_card_data(file_path)\n",
    "full_card_data = full_card_data.drop('Time', axis=1)\n",
    "X = full_card_data.drop('Class', axis=1).values\n",
    "y = full_card_data['Class'].values\n",
    "\n",
    "\n",
    "#%%\n",
    "\n",
    "# split data into train and test\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "X_valid, X_train = X_train_full[:28000], X_train_full[28000:]\n",
    "y_valid, y_train = y_train_full[:28000], y_train_full[28000:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Hyper Paramaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_logdir = \".\"\n",
    "\n",
    "# tensor board\n",
    "run_logdir = get_run_logdir()\n",
    "\n",
    "hp_units = hp_api.HParam('num_units', hp_api.Discrete([16, 32]))\n",
    "hp_drop_out = hp_api.HParam('dropout', hp_api.RealInterval(0.1, 0.2))\n",
    "hp_optimiser = hp_api.HParam('optimizer', hp_api.Discrete(['adam', 'sgd']))\n",
    "\n",
    "\n",
    "with tf.summary.create_file_writer(run_logdir + '/hparam_tuning').as_default():\n",
    "  hp_api.hparams_config(\n",
    "    hparams = [hp_units, hp_drop_out, hp_optimiser],\n",
    "    metrics=[hp_api.Metric('accuracy', display_name='Accuracy')],\n",
    "  )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model with Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial: run-0\n",
      "{'num_units': 16, 'dropout': 0.1, 'optimizer': 'adam'}\n",
      "Train on 199845 samples, validate on 28000 samples\n",
      "199845/199845 [==============================] - 16s 82us/sample - loss: 0.0016 - accuracy: 0.9983 - val_loss: 4.9904e-04 - val_accuracy: 0.9994\n",
      "56962/56962 [==============================] - 2s 38us/sample - loss: 6.7892e-04 - accuracy: 0.9992\n",
      "--- Starting trial: run-1\n",
      "{'num_units': 16, 'dropout': 0.1, 'optimizer': 'sgd'}\n",
      "Train on 199845 samples, validate on 28000 samples\n",
      "199845/199845 [==============================] - 15s 76us/sample - loss: 0.0066 - accuracy: 0.9963 - val_loss: 0.0020 - val_accuracy: 0.9985\n",
      "56962/56962 [==============================] - 2s 40us/sample - loss: 0.0023 - accuracy: 0.9981\n",
      "--- Starting trial: run-2\n",
      "{'num_units': 16, 'dropout': 0.2, 'optimizer': 'adam'}\n",
      "Train on 199845 samples, validate on 28000 samples\n",
      "199845/199845 [==============================] - 16s 80us/sample - loss: 0.0012 - accuracy: 0.9988 - val_loss: 5.3805e-04 - val_accuracy: 0.9995\n",
      "56962/56962 [==============================] - 3s 45us/sample - loss: 6.8784e-04 - accuracy: 0.9993\n",
      "--- Starting trial: run-3\n",
      "{'num_units': 16, 'dropout': 0.2, 'optimizer': 'sgd'}\n",
      "Train on 199845 samples, validate on 28000 samples\n",
      "199845/199845 [==============================] - 15s 74us/sample - loss: 0.0058 - accuracy: 0.9967 - val_loss: 0.0018 - val_accuracy: 0.9987\n",
      "56962/56962 [==============================] - 2s 40us/sample - loss: 0.0022 - accuracy: 0.9983\n",
      "--- Starting trial: run-4\n",
      "{'num_units': 32, 'dropout': 0.1, 'optimizer': 'adam'}\n",
      "Train on 199845 samples, validate on 28000 samples\n",
      "199845/199845 [==============================] - 16s 81us/sample - loss: 0.0018 - accuracy: 0.9979 - val_loss: 5.4439e-04 - val_accuracy: 0.9994\n",
      "56962/56962 [==============================] - 2s 38us/sample - loss: 7.3088e-04 - accuracy: 0.9992\n",
      "--- Starting trial: run-5\n",
      "{'num_units': 32, 'dropout': 0.1, 'optimizer': 'sgd'}\n",
      "Train on 199845 samples, validate on 28000 samples\n",
      "199845/199845 [==============================] - 15s 74us/sample - loss: 0.0064 - accuracy: 0.9951 - val_loss: 0.0019 - val_accuracy: 0.9987\n",
      "56962/56962 [==============================] - 2s 39us/sample - loss: 0.0023 - accuracy: 0.9983\n",
      "--- Starting trial: run-6\n",
      "{'num_units': 32, 'dropout': 0.2, 'optimizer': 'adam'}\n",
      "Train on 199845 samples, validate on 28000 samples\n",
      "199845/199845 [==============================] - 16s 82us/sample - loss: 0.0012 - accuracy: 0.9989 - val_loss: 5.4208e-04 - val_accuracy: 0.9994\n",
      "56962/56962 [==============================] - 2s 39us/sample - loss: 7.0938e-04 - accuracy: 0.9992\n",
      "--- Starting trial: run-7\n",
      "{'num_units': 32, 'dropout': 0.2, 'optimizer': 'sgd'}\n",
      "Train on 199845 samples, validate on 28000 samples\n",
      "199845/199845 [==============================] - 15s 75us/sample - loss: 0.0065 - accuracy: 0.9955 - val_loss: 0.0020 - val_accuracy: 0.9986\n",
      "56962/56962 [==============================] - 2s 41us/sample - loss: 0.0024 - accuracy: 0.9979\n"
     ]
    }
   ],
   "source": [
    "def build_model(hparams, logdir):\n",
    "    # build a multi layer network\n",
    "    model = keras.models.Sequential()\n",
    "\n",
    "    model.add(keras.layers.Dense(100, input_dim=29, activation=\"tanh\"))\n",
    "    model.add(keras.layers.Dense(hparams[hp_units], activation=\"tanh\"))\n",
    "    model.add(keras.layers.Dropout(rate=hparams[hp_drop_out]))\n",
    "    model.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(optimizer=hparams[hp_optimiser],\n",
    "                  loss='mean_squared_error', metrics=[\"accuracy\"])\n",
    "\n",
    "    tensorboard_cb = keras.callbacks.TensorBoard(logdir)\n",
    "    history = model.fit(X_train, y_train, epochs=1,\n",
    "                        validation_data=(X_valid, y_valid),\n",
    "                        callbacks=[tensorboard_cb,\n",
    "                                   hp_api.KerasCallback(logdir, hparams)])\n",
    "    _, accuracy = model.evaluate(X_test, y_test)\n",
    "    return accuracy\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "#%%\n",
    "\n",
    "def run(hparams, logdir):\n",
    "  with tf.summary.create_file_writer(logdir).as_default():\n",
    "    hp_api.hparams(hparams)  # record the values used in this trial\n",
    "    accuracy = build_model(hparams, logdir)\n",
    "    tf.summary.scalar('accuracy', accuracy, step=1)\n",
    "\n",
    "#%%\n",
    "\n",
    "session_num = 0\n",
    "\n",
    "for num_units in hp_units.domain.values:\n",
    "  for dropout_rate in (hp_drop_out.domain.min_value, hp_drop_out.domain.max_value):\n",
    "    for optimizer in hp_optimiser.domain.values:\n",
    "      hparams = {\n",
    "          hp_units: num_units,\n",
    "          hp_drop_out: dropout_rate,\n",
    "          hp_optimiser: optimizer,\n",
    "      }\n",
    "      run_name = \"run-%d\" % session_num\n",
    "      print('--- Starting trial: %s' % run_name)\n",
    "      print({h.name: hparams[h] for h in hparams})\n",
    "      run(hparams, run_logdir + '/hparam_tuning/' + run_name)\n",
    "      session_num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload Model to Visualise its internals. The visualisation server is hosted on OpenShift Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_2020_05_01-05_14_58/hparam_tuning/events.out.tfevents.1588310098.jupyterhub-nb-fmasood.506.5.v2\n",
      "run_2020_05_01-05_14_58/hparam_tuning/run-5/events.out.tfevents.1588310193.jupyterhub-nb-fmasood.506.264235.v2\n",
      "run_2020_05_01-05_14_58/hparam_tuning/run-5/events.out.tfevents.1588310193.jupyterhub-nb-fmasood.506.264063.v2\n",
      "run_2020_05_01-05_14_58/hparam_tuning/run-5/validation/events.out.tfevents.1588310208.jupyterhub-nb-fmasood.506.311255.v2\n",
      "run_2020_05_01-05_14_58/hparam_tuning/run-5/train/events.out.tfevents.1588310194.jupyterhub-nb-fmasood.profile-empty\n",
      "run_2020_05_01-05_14_58/hparam_tuning/run-5/train/events.out.tfevents.1588310193.jupyterhub-nb-fmasood.506.264344.v2\n",
      "run_2020_05_01-05_14_58/hparam_tuning/run-5/train/plugins/profile/2020-05-01_05-16-34/local.trace\n",
      "run_2020_05_01-05_14_58/hparam_tuning/run-2/events.out.tfevents.1588310138.jupyterhub-nb-fmasood.506.105780.v2\n",
      "run_2020_05_01-05_14_58/hparam_tuning/run-2/events.out.tfevents.1588310138.jupyterhub-nb-fmasood.506.105608.v2\n",
      "run_2020_05_01-05_14_58/hparam_tuning/run-2/validation/events.out.tfevents.1588310155.jupyterhub-nb-fmasood.506.152955.v2\n",
      "run_2020_05_01-05_14_58/hparam_tuning/run-2/train/events.out.tfevents.1588310139.jupyterhub-nb-fmasood.profile-empty\n",
      "run_2020_05_01-05_14_58/hparam_tuning/run-2/train/events.out.tfevents.1588310138.jupyterhub-nb-fmasood.506.105889.v2\n",
      "run_2020_05_01-05_14_58/hparam_tuning/run-2/train/plugins/profile/2020-05-01_05-15-39/local.trace\n",
      "run_2020_05_01-05_14_58/hparam_tuning/run-1/events.out.tfevents.1588310121.jupyterhub-nb-fmasood.506.53065.v2\n",
      "run_2020_05_01-05_14_58/hparam_tuning/run-1/events.out.tfevents.1588310121.jupyterhub-nb-fmasood.506.52893.v2\n",
      "run_2020_05_01-05_14_58/hparam_tuning/run-1/validation/events.out.tfevents.1588310136.jupyterhub-nb-fmasood.506.100085.v2\n",
      "run_2020_05_01-05_14_58/hparam_tuning/run-1/train/events.out.tfevents.1588310121.jupyterhub-nb-fmasood.profile-empty\n",
      "run_2020_05_01-05_14_58/hparam_tuning/run-1/train/events.out.tfevents.1588310121.jupyterhub-nb-fmasood.506.53174.v2\n",
      "run_2020_05_01-05_14_58/hparam_tuning/run-1/train/plugins/profile/2020-05-01_05-15-21/local.trace\n",
      "run_2020_05_01-05_14_58/hparam_tuning/run-0/events.out.tfevents.1588310102.jupyterhub-nb-fmasood.506.20.v2\n",
      "run_2020_05_01-05_14_58/hparam_tuning/run-0/events.out.tfevents.1588310102.jupyterhub-nb-fmasood.506.195.v2\n",
      "run_2020_05_01-05_14_58/hparam_tuning/run-0/validation/events.out.tfevents.1588310118.jupyterhub-nb-fmasood.506.47370.v2\n",
      "run_2020_05_01-05_14_58/hparam_tuning/run-0/train/events.out.tfevents.1588310102.jupyterhub-nb-fmasood.506.304.v2\n",
      "run_2020_05_01-05_14_58/hparam_tuning/run-0/train/events.out.tfevents.1588310102.jupyterhub-nb-fmasood.profile-empty\n",
      "run_2020_05_01-05_14_58/hparam_tuning/run-0/train/plugins/profile/2020-05-01_05-15-02/local.trace\n",
      "run_2020_05_01-05_14_58/hparam_tuning/run-3/events.out.tfevents.1588310157.jupyterhub-nb-fmasood.506.158478.v2\n",
      "run_2020_05_01-05_14_58/hparam_tuning/run-3/events.out.tfevents.1588310157.jupyterhub-nb-fmasood.506.158650.v2\n",
      "run_2020_05_01-05_14_58/hparam_tuning/run-3/validation/events.out.tfevents.1588310172.jupyterhub-nb-fmasood.506.205670.v2\n",
      "run_2020_05_01-05_14_58/hparam_tuning/run-3/train/events.out.tfevents.1588310158.jupyterhub-nb-fmasood.profile-empty\n",
      "run_2020_05_01-05_14_58/hparam_tuning/run-3/train/events.out.tfevents.1588310157.jupyterhub-nb-fmasood.506.158759.v2\n",
      "run_2020_05_01-05_14_58/hparam_tuning/run-3/train/plugins/profile/2020-05-01_05-15-58/local.trace\n",
      "run_2020_05_01-05_14_58/hparam_tuning/run-4/events.out.tfevents.1588310175.jupyterhub-nb-fmasood.506.211365.v2\n",
      "run_2020_05_01-05_14_58/hparam_tuning/run-4/events.out.tfevents.1588310175.jupyterhub-nb-fmasood.506.211193.v2\n",
      "run_2020_05_01-05_14_58/hparam_tuning/run-4/validation/events.out.tfevents.1588310191.jupyterhub-nb-fmasood.506.258540.v2\n",
      "run_2020_05_01-05_14_58/hparam_tuning/run-4/train/events.out.tfevents.1588310175.jupyterhub-nb-fmasood.506.211474.v2\n",
      "run_2020_05_01-05_14_58/hparam_tuning/run-4/train/events.out.tfevents.1588310175.jupyterhub-nb-fmasood.profile-empty\n",
      "run_2020_05_01-05_14_58/hparam_tuning/run-4/train/plugins/profile/2020-05-01_05-16-15/local.trace\n",
      "run_2020_05_01-05_14_58/hparam_tuning/run-7/events.out.tfevents.1588310229.jupyterhub-nb-fmasood.506.369648.v2\n",
      "run_2020_05_01-05_14_58/hparam_tuning/run-7/events.out.tfevents.1588310230.jupyterhub-nb-fmasood.506.369820.v2\n",
      "run_2020_05_01-05_14_58/hparam_tuning/run-7/validation/events.out.tfevents.1588310245.jupyterhub-nb-fmasood.506.416840.v2\n",
      "run_2020_05_01-05_14_58/hparam_tuning/run-7/train/events.out.tfevents.1588310230.jupyterhub-nb-fmasood.506.369929.v2\n",
      "run_2020_05_01-05_14_58/hparam_tuning/run-7/train/events.out.tfevents.1588310230.jupyterhub-nb-fmasood.profile-empty\n",
      "run_2020_05_01-05_14_58/hparam_tuning/run-7/train/plugins/profile/2020-05-01_05-17-10/local.trace\n",
      "run_2020_05_01-05_14_58/hparam_tuning/run-6/events.out.tfevents.1588310210.jupyterhub-nb-fmasood.506.316778.v2\n",
      "run_2020_05_01-05_14_58/hparam_tuning/run-6/events.out.tfevents.1588310211.jupyterhub-nb-fmasood.506.316950.v2\n",
      "run_2020_05_01-05_14_58/hparam_tuning/run-6/validation/events.out.tfevents.1588310227.jupyterhub-nb-fmasood.506.364125.v2\n",
      "run_2020_05_01-05_14_58/hparam_tuning/run-6/train/events.out.tfevents.1588310211.jupyterhub-nb-fmasood.profile-empty\n",
      "run_2020_05_01-05_14_58/hparam_tuning/run-6/train/events.out.tfevents.1588310211.jupyterhub-nb-fmasood.506.317059.v2\n",
      "run_2020_05_01-05_14_58/hparam_tuning/run-6/train/plugins/profile/2020-05-01_05-16-51/local.trace\n"
     ]
    }
   ],
   "source": [
    "upload_learning_stats_to_s3(run_logdir.replace(\"./\", \"\", 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}