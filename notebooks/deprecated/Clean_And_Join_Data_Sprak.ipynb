{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JupyterHub Notebook\n",
    "\n",
    "### This notebook server is hosted on the OpenShift platform which provides a separate server for individual user. The platform take care about the provisioning of the server and allocating related to storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import from_json, col, to_json, struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSessionBuilder = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Credit card data ingest Pipeline\") \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \\\n",
    "'--packages \\\n",
    "org.postgresql:postgresql:42.2.10,\\\n",
    "org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5,\\\n",
    "org.apache.kafka:kafka-clients:2.4.0,\\\n",
    "org.apache.spark:spark-streaming_2.11:2.4.5,\\\n",
    "org.apache.hadoop:hadoop-aws:2.7.3 \\\n",
    "--conf spark.jars.ivy=/tmp \\\n",
    "--conf spark.hadoop.fs.s3a.endpoint=http://172.30.226.86:9000 \\\n",
    "--conf spark.hadoop.fs.s3a.access.key=minio \\\n",
    "--conf spark.hadoop.fs.s3a.secret.key=minio123 \\\n",
    "--conf spark.hadoop.fs.s3a.path.style.access=true \\\n",
    "--conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \\\n",
    "--master spark://' + os.environ['SPARK_CLUSTER'] + ':7077 pyspark-shell '\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to Spark Cluster provided by OpenShift Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark context started.\n"
     ]
    }
   ],
   "source": [
    "spark = sparkSessionBuilder.getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "print('Spark context started.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch Data from CRM Database - Hosted by OpenShift Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using JDBC number of partitions : 200\n",
      "Creating dataframe...\n",
      "Dataframe created.\n"
     ]
    }
   ],
   "source": [
    "### READ FROM POSTGRES ###\n",
    "postgresUrl = \"jdbc:postgresql://postgres-ml-workshop:5432/ml-workshop\"\n",
    "tableName =  \"creditcard\"\n",
    "dbUser = \"postgres\"\n",
    "dbPassword = \"postgres\"\n",
    "numberOfPartitions = 200\n",
    "partitionColumn = \"row_number\"\n",
    "\n",
    "\n",
    "def getUpperBound(sqlContext, postgresUrl, tableName, dbUser, dbPassword):\n",
    "    # Read from postgres, count the number of lines. We need to know how many lines are there for partitioning.\n",
    "    upperBound = sqlContext.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", postgresUrl) \\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "        .option(\"dbtable\", \"(select count(*) as result from \" + tableName + \") as upperBound\")\\\n",
    "        .option(\"user\", dbUser)\\\n",
    "        .option(\"password\", dbPassword)\\\n",
    "        .load().head()[0]\n",
    "    \n",
    "    return upperBound\n",
    "\n",
    "\n",
    "# Read postrgres data into a dataframe\n",
    "sqlContext = SQLContext(spark.sparkContext)\n",
    "reader = sqlContext.read \\\n",
    "    .format(\"jdbc\")\\\n",
    "    .option(\"url\", postgresUrl)\\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .option(\"dbtable\", tableName) \\\n",
    "    .option(\"user\", dbUser)\\\n",
    "    .option(\"password\", dbPassword)\n",
    "\n",
    "upperBound = getUpperBound(sqlContext, postgresUrl, tableName, dbUser, dbPassword)\n",
    "if (upperBound == 0):\n",
    "    print(\"The JDBC source is empty.\")\n",
    "\n",
    "if numberOfPartitions > 0:\n",
    "    print(f\"Using JDBC number of partitions : {numberOfPartitions}\")\n",
    "    reader = reader.option(\"partitionColumn\", partitionColumn) \\\n",
    "        .option(\"lowerbound\", 0) \\\n",
    "        .option(\"upperBound\", upperBound) \\\n",
    "        .option(\"numPartitions\", numberOfPartitions)\n",
    "\n",
    "print(\"Creating dataframe...\")\n",
    "dfPostgres = reader.load()\n",
    "print(\"Dataframe created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch Data from Read Time Kafka Cluster - Provided by OpenShift Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Kafka JSON messages...\n",
      "Dataframe created.\n"
     ]
    }
   ],
   "source": [
    "srcKafkaBrokers = \"kafka-ml-workshop-kafka-bootstrap:9092\"\n",
    "srcKakaTopic = \"mlworkshop.creditcard\"\n",
    "    \n",
    "#Read from JSON Kafka messages into a dataframe\n",
    "dfKafka = spark.read.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", srcKafkaBrokers) \\\n",
    "    .option(\"subscribe\", srcKakaTopic) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load() \\\n",
    "    .selectExpr(\"CAST(value AS STRING) as jsonValue\") \\\n",
    "    .rdd.map(lambda row: row[\"jsonValue\"])\n",
    "\n",
    "#print(f\"Source Kafka messages count: {dfKafka.count()}\")\n",
    "print(\"Loading Kafka JSON messages...\")\n",
    "dfObj = spark.read.json(dfKafka)\n",
    "print(\"Dataframe created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join the data from CRM and Real Time Kafka and store it in S3. S3 server is provided by OpenShift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema after join.\n",
      "root\n",
      " |-- row_number: long (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- V1: string (nullable = true)\n",
      " |-- V2: string (nullable = true)\n",
      " |-- V3: string (nullable = true)\n",
      " |-- V4: string (nullable = true)\n",
      " |-- V5: string (nullable = true)\n",
      " |-- V6: string (nullable = true)\n",
      " |-- V7: string (nullable = true)\n",
      " |-- V8: string (nullable = true)\n",
      " |-- V9: string (nullable = true)\n",
      " |-- V10: string (nullable = true)\n",
      " |-- V11: string (nullable = true)\n",
      " |-- V12: string (nullable = true)\n",
      " |-- V13: string (nullable = true)\n",
      " |-- V14: string (nullable = true)\n",
      " |-- V15: string (nullable = true)\n",
      " |-- V16: string (nullable = true)\n",
      " |-- V17: string (nullable = true)\n",
      " |-- Amount: string (nullable = true)\n",
      " |-- Class: string (nullable = true)\n",
      " |-- V18: string (nullable = true)\n",
      " |-- V19: string (nullable = true)\n",
      " |-- V20: string (nullable = true)\n",
      " |-- V21: string (nullable = true)\n",
      " |-- V22: string (nullable = true)\n",
      " |-- V23: string (nullable = true)\n",
      " |-- V24: string (nullable = true)\n",
      " |-- V25: string (nullable = true)\n",
      " |-- V26: string (nullable = true)\n",
      " |-- V27: string (nullable = true)\n",
      " |-- V28: string (nullable = true)\n",
      "\n",
      "Writing joined dataframe to S3 bucket...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "### DO A FULL OUTER JOIN ON 2 DATAFRAMES AND WRITE TO S3 BUCKET ###\n",
    "dfJoined = dfPostgres.join(dfObj, \"row_number\", how=\"full\")\n",
    "\n",
    "print(\"Schema after join.\")\n",
    "dfJoined.printSchema()\n",
    "\n",
    "print(\"Writing joined dataframe to S3 bucket...\")\n",
    "dfJoined.write.mode(\"overwrite\").format(\"csv\").save(\"s3a://data/credticard_clean_csv\")\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STOP SPARK SESSION ###\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}