{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection Predictive Models\n",
    "## Introduction\n",
    "The dataset contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
    "\n",
    "It contains only numerical input variables which are the result of a PCA transformation.\n",
    "\n",
    "Due to confidentiality issues, there are not provided the original features and more background information about the data.\n",
    "\n",
    "Features V1, V2, ... V28 are the principal components obtained with PCA;\n",
    "The only features which have not been transformed with PCA are Time and Amount. Feature Time contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature Amount is the transaction Amount, this feature can be used for example-dependant cost-senstive learning.\n",
    "\n",
    "Feature Class is the response variable and it takes value 1 in case of fraud and 0 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install plotly catboost minio lightgbm xgboost seaborn uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline \n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "from plotly import tools\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "\n",
    "import gc\n",
    "from datetime import datetime \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn import svm\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "from minio import Minio\n",
    "from minio.error import ResponseError\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "\n",
    "RFC_METRIC = 'gini'  #metric used for RandomForrestClassifier\n",
    "NUM_ESTIMATORS = 100 #number of estimators used for RandomForrestClassifier\n",
    "NO_JOBS = 4 #number of parallel jobs used for RandomForrestClassifier\n",
    "\n",
    "\n",
    "#TRAIN/VALIDATION/TEST SPLIT\n",
    "#VALIDATION\n",
    "VALID_SIZE = 0.20 # simple validation using train_test_split\n",
    "TEST_SIZE = 0.20 # test size using_train_test_split\n",
    "\n",
    "#CROSS-VALIDATION\n",
    "NUMBER_KFOLDS = 5 #number of KFolds for cross-validation\n",
    "\n",
    "\n",
    "\n",
    "RANDOM_STATE = 2018\n",
    "\n",
    "MAX_ROUNDS = 1000 #lgb iterations\n",
    "EARLY_STOP = 50 #lgb early stop \n",
    "OPT_ROUNDS = 1000  #To be adjusted based on best validation rounds\n",
    "VERBOSE_EVAL = 50 #Print out metric result\n",
    "\n",
    "IS_LOCAL = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_s3_server():\n",
    "    minioClient = Minio('minio-ml-workshop:9000',\n",
    "                    access_key='minio',\n",
    "                    secret_key='minio123',\n",
    "                    secure=False)\n",
    "\n",
    "    return minioClient\n",
    "\n",
    "def download_all_files(bucket_name):\n",
    "    minioClient = get_s3_server()\n",
    "    objects = minioClient.list_objects_v2(bucket_name=bucket_name,\n",
    "                                          recursive=True)\n",
    "    for obj in objects:\n",
    "        print(obj.bucket_name, obj.object_name.encode('utf-8'), obj.last_modified,\n",
    "              obj.etag, obj.size, obj.content_type)\n",
    "        try:\n",
    "            print(minioClient.fget_object(obj.bucket_name, obj.object_name,\n",
    "                                          '/tmp/' + os.path.basename(obj.object_name)))\n",
    "        except ResponseError as err:\n",
    "            print(err)\n",
    "\n",
    "#%%\n",
    "\n",
    "def load_card_data(file_path):\n",
    "    csv_path = os.path.join(file_path, \"creditcard.csv\")\n",
    "    # return pd.read_csv(csv_path, header=None)\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "#%%\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "def upload_learning_stats_to_s3(folder_name):\n",
    "    minioClient = get_s3_server()\n",
    "\n",
    "    files = []\n",
    "    for r, d, f in os.walk(folder_name):\n",
    "        for file in f:\n",
    "            files.append(os.path.join(r, file))\n",
    "\n",
    "    for f in files:\n",
    "        print(f)\n",
    "        minioClient.fput_object(bucket_name='model-stats', object_name=\"tensordata/\"  + f , file_path='./' + f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locally\n",
    "#print(os.listdir(\"../data\"))\n",
    "#data_df = pd.read_csv(\"../data/creditcard.csv\")\n",
    "\n",
    "# from s3\n",
    "download_all_files('rawdata')\n",
    "file_path = \"/tmp\"\n",
    "data_df = load_card_data(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Credit Card Fraud Detection data -  rows:\",data_df.shape[0],\" columns:\", data_df.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glimpse the data\n",
    "We start by looking to the data features (first 5 rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look into more details to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking to the Time feature, we can confirm that the data contains 284,807 transactions, during 2 consecutive days (or 172792 seconds)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check missing data\n",
    "Let's check if there is any missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = data_df.isnull().sum().sort_values(ascending = False)\n",
    "percent = (data_df.isnull().sum()/data_df.isnull().count()*100).sort_values(ascending = False)\n",
    "pd.concat([total, percent], axis=1, keys=['Total', 'Percent']).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no missing data in the entire dataset.\n",
    "\n",
    "## Data unbalance\n",
    "Let's check data unbalance with respect with target value, i.e. Class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = data_df[\"Class\"].value_counts()\n",
    "df = pd.DataFrame({'Class': temp.index,'values': temp.values})\n",
    "\n",
    "trace = go.Bar(\n",
    "    x = df['Class'],y = df['values'],\n",
    "    name=\"Credit Card Fraud Class - data unbalance (Not fraud = 0, Fraud = 1)\",\n",
    "    marker=dict(color=\"Red\"),\n",
    "    text=df['values']\n",
    ")\n",
    "data = [trace]\n",
    "layout = dict(title = 'Credit Card Fraud Class - data unbalance (Not fraud = 0, Fraud = 1)',\n",
    "          xaxis = dict(title = 'Class', showticklabels=True), \n",
    "          yaxis = dict(title = 'Number of transactions'),\n",
    "          hovermode = 'closest',width=600\n",
    "         )\n",
    "fig = dict(data=data, layout=layout)\n",
    "iplot(fig, filename='class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 492 (or 0.172%) of transaction are fraudulent. That means the data is highly unbalanced with respect with target variable Class.\n",
    "\n",
    "## Data exploration\n",
    "### Transactions in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_0 = data_df.loc[data_df['Class'] == 0][\"Time\"]\n",
    "class_1 = data_df.loc[data_df['Class'] == 1][\"Time\"]\n",
    "#plt.figure(figsize = (14,4))\n",
    "#plt.title('Credit Card Transactions Time Density Plot')\n",
    "#sns.set_color_codes(\"pastel\")\n",
    "#sns.distplot(class_0,kde=True,bins=480)\n",
    "#sns.distplot(class_1,kde=True,bins=480)\n",
    "#plt.show()\n",
    "hist_data = [class_0, class_1]\n",
    "group_labels = ['Not Fraud', 'Fraud']\n",
    "\n",
    "fig = ff.create_distplot(hist_data, group_labels, show_hist=False, show_rug=False)\n",
    "fig['layout'].update(title='Credit Card Transactions Time Density Plot', xaxis=dict(title='Time [s]'))\n",
    "iplot(fig, filename='dist_only')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fraudulent transactions have a distribution more even than valid transactions - are equaly distributed in time, including the low real transaction times, during night in Europe timezone.\n",
    "\n",
    "## Transactions amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))\n",
    "s = sns.boxplot(ax = ax1, x=\"Class\", y=\"Amount\", hue=\"Class\",data=data_df, palette=\"PRGn\",showfliers=True)\n",
    "s = sns.boxplot(ax = ax2, x=\"Class\", y=\"Amount\", hue=\"Class\",data=data_df, palette=\"PRGn\",showfliers=False)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = data_df[['Amount','Class']].copy()\n",
    "class_0 = tmp.loc[tmp['Class'] == 0]['Amount']\n",
    "class_1 = tmp.loc[tmp['Class'] == 1]['Amount']\n",
    "class_0.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The real transaction have a larger mean value, larger Q1, smaller Q3 and Q4 and larger outliers; fraudulent transactions have a smaller Q1 and mean, larger Q4 and smaller outliers.\n",
    "\n",
    "Let's plot the fraudulent transactions (amount) against time. The time is shown is seconds from the start of the time period (totaly 48h, over 2 days)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud = data_df.loc[data_df['Class'] == 1]\n",
    "\n",
    "trace = go.Scatter(\n",
    "    x = fraud['Time'],y = fraud['Amount'],\n",
    "    name=\"Amount\",\n",
    "     marker=dict(\n",
    "                color='rgb(238,23,11)',\n",
    "                line=dict(\n",
    "                    color='red',\n",
    "                    width=1),\n",
    "                opacity=0.5,\n",
    "            ),\n",
    "    text= fraud['Amount'],\n",
    "    mode = \"markers\"\n",
    ")\n",
    "data = [trace]\n",
    "layout = dict(title = 'Amount of fraudulent transactions',\n",
    "          xaxis = dict(title = 'Time [s]', showticklabels=True), \n",
    "          yaxis = dict(title = 'Amount'),\n",
    "          hovermode='closest'\n",
    "         )\n",
    "fig = dict(data=data, layout=layout)\n",
    "iplot(fig, filename='fraud-amount')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14,14))\n",
    "plt.title('Credit Card Transactions features correlation plot (Pearson)')\n",
    "corr = data_df.corr()\n",
    "sns.heatmap(corr,xticklabels=corr.columns,yticklabels=corr.columns,linewidths=.1,cmap=\"Reds\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, there is no notable correlation between features V1-V28. There are certain correlations between some of these features and Time (inverse correlation with V3) and Amount (direct correlation with V7 and V20, inverse correlation with V1 and V5).\n",
    "\n",
    "Let's plot the correlated and inverse correlated values on the same graph.\n",
    "\n",
    "Let's start with the direct correlated values: {V20;Amount} and {V7;Amount}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = sns.lmplot(x='V20', y='Amount',data=data_df, hue='Class', fit_reg=True,scatter_kws={'s':2})\n",
    "s = sns.lmplot(x='V7', y='Amount',data=data_df, hue='Class', fit_reg=True,scatter_kws={'s':2})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm that the two couples of features are correlated (the regression lines for Class = 0 have a positive slope, whilst the regression line for Class = 1 have a smaller positive slope).\n",
    "\n",
    "Let's plot now the inverse correlated values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = sns.lmplot(x='V2', y='Amount',data=data_df, hue='Class', fit_reg=True,scatter_kws={'s':2})\n",
    "s = sns.lmplot(x='V5', y='Amount',data=data_df, hue='Class', fit_reg=True,scatter_kws={'s':2})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm that the two couples of features are inverse correlated (the regression lines for Class = 0 have a negative slope while the regression lines for Class = 1 have a very small negative slope)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features density plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = data_df.columns.values\n",
    "\n",
    "i = 0\n",
    "t0 = data_df.loc[data_df['Class'] == 0]\n",
    "t1 = data_df.loc[data_df['Class'] == 1]\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.figure()\n",
    "fig, ax = plt.subplots(8,4,figsize=(16,28))\n",
    "\n",
    "for feature in var:\n",
    "    i += 1\n",
    "    plt.subplot(8,4,i)\n",
    "    sns.kdeplot(t0[feature], bw=0.5,label=\"Class = 0\")\n",
    "    sns.kdeplot(t1[feature], bw=0.5,label=\"Class = 1\")\n",
    "    plt.xlabel(feature, fontsize=12)\n",
    "    locs, labels = plt.xticks()\n",
    "    plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some of the features we can observe a good selectivity in terms of distribution for the two values of Class: V4, V11 have clearly separated distributions for Class values 0 and 1, V12, V14, V18 are partially separated, V1, V2, V3, V10 have a quite distinct profile, whilst V25, V26, V28 have similar profiles for the two values of Class.\n",
    "\n",
    "In general, with just few exceptions (Time and Amount), the features distribution for legitimate transactions (values of Class = 0) is centered around 0, sometime with a long queue at one of the extremities. In the same time, the fraudulent transactions (values of Class = 1) have a skewed (asymmetric) distribution.\n",
    "\n",
    "## Predictive models\n",
    "\n",
    "### Define predictors and target values\n",
    "Let's define the predictor features and the target features. Categorical features, if any, are also defined. In our case, there are no categorical feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'Class'\n",
    "predictors = ['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\\\n",
    "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19',\\\n",
    "       'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28',\\\n",
    "       'Amount']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data in train, test and validation set\n",
    "Let's define train, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(data_df, test_size=TEST_SIZE, random_state=RANDOM_STATE, shuffle=True )\n",
    "train_df, valid_df = train_test_split(train_df, test_size=VALID_SIZE, random_state=RANDOM_STATE, shuffle=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a RandomForrestClassifier model - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForestClassifier\n",
    "### Define model parameters\n",
    "Let's set the parameters for the model.\n",
    "\n",
    "Let's run a model using the training set for training. Then, we will use the validation set for validation.\n",
    "\n",
    "We will use as validation criterion GINI, which formula is GINI = 2 * (AUC) - 1, where AUC is the Receiver Operating Characteristic - Area Under Curve (ROC-AUC) - https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve\n",
    "\n",
    "Number of estimators is set to 100 and number of parallel jobs is set to 4.\n",
    "\n",
    "We start by initializing the RandomForestClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_jobs=NO_JOBS, \n",
    "                             random_state=RANDOM_STATE,\n",
    "                             criterion=RFC_METRIC,\n",
    "                             n_estimators=NUM_ESTIMATORS,\n",
    "                             verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the RandonForestClassifier using the train_df data and fit function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(train_df[predictors], train_df[target].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now predict the target values for the valid_df data, using predict function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.predict(valid_df[predictors])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also visualize the features importance.\n",
    "\n",
    "### Features importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame({'Feature': predictors, 'Feature importance': clf.feature_importances_})\n",
    "tmp = tmp.sort_values(by='Feature importance',ascending=False)\n",
    "plt.figure(figsize = (7,4))\n",
    "plt.title('Features importance',fontsize=14)\n",
    "s = sns.barplot(x='Feature',y='Feature importance',data=tmp)\n",
    "s.set_xticklabels(s.get_xticklabels(),rotation=90)\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important features are V17, V12, V14, V10, V11, V16.\n",
    "\n",
    "### Confusion matrix\n",
    "Let's show a confusion matrix for the results we obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = pd.crosstab(valid_df[target].values, preds, rownames=['Actual'], colnames=['Predicted'])\n",
    "fig, (ax1) = plt.subplots(ncols=1, figsize=(5,5))\n",
    "sns.heatmap(cm, \n",
    "            xticklabels=['Not Fraud', 'Fraud'],\n",
    "            yticklabels=['Not Fraud', 'Fraud'],\n",
    "            annot=True,ax=ax1,\n",
    "            linewidths=.2,linecolor=\"Darkblue\", cmap=\"Blues\")\n",
    "plt.title('Confusion Matrix', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type I error and Type II error\n",
    "We need to clarify that confussion matrix are not a very good tool to represent the results in the case of largely unbalanced data, because we will actually need a different metrics that accounts in the same time for the selectivity and specificity of the method we are using, so that we minimize in the same time both Type I errors and Type II errors.\n",
    "\n",
    "Null Hypothesis (H0) - The transaction is not a fraud.\n",
    "Alternative Hypothesis (H1) - The transaction is a fraud.\n",
    "\n",
    "Type I error - You reject the null hypothesis when the null hypothesis is actually true.\n",
    "Type II error - You fail to reject the null hypothesis when the the alternative hypothesis is true.\n",
    "\n",
    "Cost of Type I error - You erroneously presume that the the transaction is a fraud, and a true transaction is rejected.\n",
    "Cost of Type II error - You erroneously presume that the transaction is not a fraud and a ffraudulent transaction is accepted.\n",
    "\n",
    "Let's calculate the ROC-AUC score - https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve\n",
    "\n",
    "### Area under curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(valid_df[target].values, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC-AUC score obtained with RandomForrestClassifier is 0.85.\n",
    "\n",
    "## AdaBoostClassifier\n",
    "AdaBoostClassifier stands for Adaptive Boosting Classifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
    "\n",
    "### Prepare the model\n",
    "Let's set the parameters for the model and initialize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = AdaBoostClassifier(random_state=RANDOM_STATE,\n",
    "                         algorithm='SAMME.R',\n",
    "                         learning_rate=0.8,\n",
    "                             n_estimators=NUM_ESTIMATORS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model\n",
    "Let's fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(train_df[predictors], train_df[target].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the target values\n",
    "Let's now predict the target values for the valid_df data, using predict function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.predict(valid_df[predictors])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features importance\n",
    "Let's see also the features importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame({'Feature': predictors, 'Feature importance': clf.feature_importances_})\n",
    "tmp = tmp.sort_values(by='Feature importance',ascending=False)\n",
    "plt.figure(figsize = (7,4))\n",
    "plt.title('Features importance',fontsize=14)\n",
    "s = sns.barplot(x='Feature',y='Feature importance',data=tmp)\n",
    "s.set_xticklabels(s.get_xticklabels(),rotation=90)\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix\n",
    "Let's visualize the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = pd.crosstab(valid_df[target].values, preds, rownames=['Actual'], colnames=['Predicted'])\n",
    "fig, (ax1) = plt.subplots(ncols=1, figsize=(5,5))\n",
    "sns.heatmap(cm, \n",
    "            xticklabels=['Not Fraud', 'Fraud'],\n",
    "            yticklabels=['Not Fraud', 'Fraud'],\n",
    "            annot=True,ax=ax1,\n",
    "            linewidths=.2,linecolor=\"Darkblue\", cmap=\"Blues\")\n",
    "plt.title('Confusion Matrix', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate also the ROC-AUC.\n",
    "\n",
    "### Area under curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(valid_df[target].values, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC-AUC score obtained with AdaBoostClassifier is 0.83.\n",
    "\n",
    "## CatBoostClassifier\n",
    "CatBoostClassifier is a gradient boosting for decision trees algorithm with support for handling categorical data - https://tech.yandex.com/catboost/doc/dg/concepts/python-reference_catboostclassifier-docpage/\n",
    "\n",
    "### Prepare the model\n",
    "Let's set the parameters for the model and initialize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = CatBoostClassifier(iterations=500,\n",
    "                             learning_rate=0.02,\n",
    "                             depth=12,\n",
    "                             eval_metric='AUC',\n",
    "                             random_seed = RANDOM_STATE,\n",
    "                             bagging_temperature = 0.2,\n",
    "                             od_type='Iter',\n",
    "                             metric_period = VERBOSE_EVAL,\n",
    "                             od_wait=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(train_df[predictors], train_df[target].values,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the target values\n",
    "Let's now predict the target values for the val_df data, using predict function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.predict(valid_df[predictors])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features importance\n",
    "Let's see also the features importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame({'Feature': predictors, 'Feature importance': clf.feature_importances_})\n",
    "tmp = tmp.sort_values(by='Feature importance',ascending=False)\n",
    "plt.figure(figsize = (7,4))\n",
    "plt.title('Features importance',fontsize=14)\n",
    "s = sns.barplot(x='Feature',y='Feature importance',data=tmp)\n",
    "s.set_xticklabels(s.get_xticklabels(),rotation=90)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix\n",
    "Let's visualize the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = pd.crosstab(valid_df[target].values, preds, rownames=['Actual'], colnames=['Predicted'])\n",
    "fig, (ax1) = plt.subplots(ncols=1, figsize=(5,5))\n",
    "sns.heatmap(cm, \n",
    "            xticklabels=['Not Fraud', 'Fraud'],\n",
    "            yticklabels=['Not Fraud', 'Fraud'],\n",
    "            annot=True,ax=ax1,\n",
    "            linewidths=.2,linecolor=\"Darkblue\", cmap=\"Blues\")\n",
    "plt.title('Confusion Matrix', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate also the ROC-AUC.\n",
    "\n",
    "### Area under curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(valid_df[target].values, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC-AUC score obtained with CatBoostClassifier is 0.86.\n",
    "\n",
    "## XGBoost\n",
    "XGBoost is a gradient boosting algorithm - http://xgboost.readthedocs.io/en/latest/python/python_api.html\n",
    "\n",
    "Let's prepare the model.\n",
    "\n",
    "### Prepare the model\n",
    "We initialize the DMatrix objects for training and validation, starting from the datasets. We also set some of the parameters used for the model tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the train and valid datasets\n",
    "dtrain = xgb.DMatrix(train_df[predictors], train_df[target].values)\n",
    "dvalid = xgb.DMatrix(valid_df[predictors], valid_df[target].values)\n",
    "dtest = xgb.DMatrix(test_df[predictors], test_df[target].values)\n",
    "\n",
    "#What to monitor (in this case, **train** and **valid**)\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "\n",
    "# Set xgboost parameters\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eta'] = 0.039\n",
    "params['silent'] = True\n",
    "params['max_depth'] = 2\n",
    "params['subsample'] = 0.8\n",
    "params['colsample_bytree'] = 0.9\n",
    "params['eval_metric'] = 'auc'\n",
    "params['random_state'] = RANDOM_STATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "Let's train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.train(params, \n",
    "                dtrain, \n",
    "                MAX_ROUNDS, \n",
    "                watchlist, \n",
    "                early_stopping_rounds=EARLY_STOP, \n",
    "                maximize=True, \n",
    "                verbose_eval=VERBOSE_EVAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best validation score (ROC-AUC) was 0.984, for round 241.\n",
    "\n",
    "### Plot variable importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "dump(model, \"xgboost.cc-aml.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax) = plt.subplots(ncols=1, figsize=(8,5))\n",
    "xgb.plot_importance(model, height=0.8, title=\"Features importance (XGBoost)\", ax=ax, color=\"green\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict test set\n",
    "We used the train and validation sets for training and validation. We will use the trained model now to predict the target value for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(dtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area under curve\n",
    "Let's calculate ROC-AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(test_df[target].values, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUC score for the prediction of fresh data (test set) is 0.974.\n",
    "\n",
    "## LightGBM\n",
    "Let's continue with another gradient boosting algorithm, LightGBM\n",
    "https://github.com/Microsoft/LightGBM/tree/master/python-package\n",
    "https://www.microsoft.com/en-us/research/wp-content/uploads/2017/11/lightgbm.pdf\n",
    "\n",
    "### Define model parameters\n",
    "Let's set the parameters for the model. We will use these parameters only for the first lgb model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "          'boosting_type': 'gbdt',\n",
    "          'objective': 'binary',\n",
    "          'metric':'auc',\n",
    "          'learning_rate': 0.05,\n",
    "          'num_leaves': 7,  # we should let it be smaller than 2^(max_depth)\n",
    "          'max_depth': 4,  # -1 means no limit\n",
    "          'min_child_samples': 100,  # Minimum number of data need in a child(min_data_in_leaf)\n",
    "          'max_bin': 100,  # Number of bucketed bin for feature values\n",
    "          'subsample': 0.9,  # Subsample ratio of the training instance.\n",
    "          'subsample_freq': 1,  # frequence of subsample, <=0 means no enable\n",
    "          'colsample_bytree': 0.7,  # Subsample ratio of columns when constructing each tree.\n",
    "          'min_child_weight': 0,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n",
    "          'min_split_gain': 0,  # lambda_l1, lambda_l2 and min_gain_to_split to regularization\n",
    "          'nthread': 8,\n",
    "          'verbose': 0,\n",
    "          'scale_pos_weight':150, # because training data is extremely unbalanced \n",
    "         }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the model\n",
    "Let's prepare the model, creating the Datasets data structures from the train and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = lgb.Dataset(train_df[predictors].values, \n",
    "                     label=train_df[target].values,\n",
    "                     feature_name=predictors)\n",
    "\n",
    "dvalid = lgb.Dataset(valid_df[predictors].values,\n",
    "                     label=valid_df[target].values,\n",
    "                     feature_name=predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model\n",
    "Let's run the model, using the train function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals_results = {}\n",
    "\n",
    "model = lgb.train(params, \n",
    "                  dtrain, \n",
    "                  valid_sets=[dtrain, dvalid], \n",
    "                  valid_names=['train','valid'], \n",
    "                  evals_result=evals_results, \n",
    "                  num_boost_round=MAX_ROUNDS,\n",
    "                  early_stopping_rounds=2*EARLY_STOP,\n",
    "                  verbose_eval=VERBOSE_EVAL, \n",
    "                  feval=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best validation score was obtained for round 85, for which AUC ~= 0.974.\n",
    "\n",
    "Let's plot variable importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax) = plt.subplots(ncols=1, figsize=(8,5))\n",
    "lgb.plot_importance(model, height=0.8, title=\"Features importance (LightGBM)\", ax=ax,color=\"red\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's predict now the target for the test data.\n",
    "\n",
    "### Predict test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(test_df[predictors])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Area under curve\n",
    "Let's calculate the ROC-AUC score for the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(test_df[target].values, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC-AUC score obtained for the test set is 0.946.\n",
    "\n",
    "## Training and validation using cross-validation\n",
    "Let's use now cross-validation. We will use cross-validation (KFolds) with 5 folds. Data is divided in 5 folds and, by rotation, we are training using 4 folds (n-1) and validate using the 5th (nth) fold.\n",
    "\n",
    "Test set is calculated as an average of the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits = NUMBER_KFOLDS, random_state = RANDOM_STATE, shuffle = True)\n",
    "\n",
    "# Create arrays and dataframes to store results\n",
    "oof_preds = np.zeros(train_df.shape[0])\n",
    "test_preds = np.zeros(test_df.shape[0])\n",
    "feature_importance_df = pd.DataFrame()\n",
    "n_fold = 0\n",
    "for train_idx, valid_idx in kf.split(train_df):\n",
    "    train_x, train_y = train_df[predictors].iloc[train_idx],train_df[target].iloc[train_idx]\n",
    "    valid_x, valid_y = train_df[predictors].iloc[valid_idx],train_df[target].iloc[valid_idx]\n",
    "    \n",
    "    evals_results = {}\n",
    "    model =  LGBMClassifier(\n",
    "                  nthread=-1,\n",
    "                  n_estimators=2000,\n",
    "                  learning_rate=0.01,\n",
    "                  num_leaves=80,\n",
    "                  colsample_bytree=0.98,\n",
    "                  subsample=0.78,\n",
    "                  reg_alpha=0.04,\n",
    "                  reg_lambda=0.073,\n",
    "                  subsample_for_bin=50,\n",
    "                  boosting_type='gbdt',\n",
    "                  is_unbalance=False,\n",
    "                  min_split_gain=0.025,\n",
    "                  min_child_weight=40,\n",
    "                  min_child_samples=510,\n",
    "                  objective='binary',\n",
    "                  metric='auc',\n",
    "                  silent=-1,\n",
    "                  verbose=-1,\n",
    "                  feval=None)\n",
    "    model.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], \n",
    "                eval_metric= 'auc', verbose= VERBOSE_EVAL, early_stopping_rounds= EARLY_STOP)\n",
    "    \n",
    "    oof_preds[valid_idx] = model.predict_proba(valid_x, num_iteration=model.best_iteration_)[:, 1]\n",
    "    test_preds += model.predict_proba(test_df[predictors], num_iteration=model.best_iteration_)[:, 1] / kf.n_splits\n",
    "    \n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = predictors\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importances_\n",
    "    fold_importance_df[\"fold\"] = n_fold + 1\n",
    "    \n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx])))\n",
    "    del model, train_x, train_y, valid_x, valid_y\n",
    "    gc.collect()\n",
    "    n_fold = n_fold + 1\n",
    "train_auc_score = roc_auc_score(train_df[target], oof_preds)\n",
    "print('Full AUC score %.6f' % train_auc_score)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUC score for the prediction from the test data was 0.93.\n",
    "\n",
    "We prepare the test prediction, from the averaged predictions for test over the 5 folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(test_df[target].values, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "We investigated the data, checking for data unbalancing, visualizing the features and understanding the relationship between different features. We then investigated two predictive models. The data was split in 3 parts, a train set, a validation set and a test set. For the first three models, we only used the train and test set.\n",
    "\n",
    "We started with RandomForrestClassifier, for which we obtained an AUC scode of 0.85 when predicting the target for the test set.\n",
    "\n",
    "We followed with an AdaBoostClassifier model, with lower AUC score (0.83) for prediction of the test set target values.\n",
    "\n",
    "We then followed with an CatBoostClassifier, with the AUC score after training 500 iterations 0.86.\n",
    "\n",
    "We then experimented with a XGBoost model. In this case, se used the validation set for validation of the training model. The best validation score obtained was 0.984. Then we used the model with the best training step, to predict target value from the test data; the AUC score obtained was 0.974.\n",
    "\n",
    "We then presented the data to a LightGBM model. We used both train-validation split and cross-validation to evaluate the model effectiveness to predict 'Class' value, i.e. detecting if a transaction was fraudulent. With the first method we obtained values of AUC for the validation set around 0.974. For the test set, the score obtained was 0.946.\n",
    "With the cross-validation, we obtained an AUC score for the test prediction of 0.93."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('keras': virtualenv)",
   "language": "python",
   "name": "python37764bitkerasvirtualenv8b8a4ebf0fdb4dbbbee0b75fcbf2ee10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}